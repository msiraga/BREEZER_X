# Production LLM Dockerfile
FROM nvidia/cuda:12.2.2-devel-ubuntu22.04

# Install dependencies
RUN apt-get update && \
    apt-get install -y git cmake libcurl4-openssl-dev && \
    rm -rf /var/lib/apt/lists/*

# Build llama.cpp
RUN git clone https://github.com/ggerganov/llama.cpp && \
    cd llama.cpp && \
    mkdir build && \
    cd build && \
    cmake .. -DGGML_CUDA=ON -DLLAMA_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release && \
    cmake --build .

# Set CUDA library paths
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:$LD_LIBRARY_PATH

# Copy model
COPY mistral.gguf /models/

# Expose port
EXPOSE 8080

# Start server
CMD ["/llama.cpp/build/bin/server", "-m", "/models/mistral.gguf", "--port", "8080"]
