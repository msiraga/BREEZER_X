# BREEZER Configuration by RICHDALE AI

# =============================================================================
# LLM API Keys
# =============================================================================

# DeepSeek (Primary for testing/building)
DEEPSEEK_API_KEY=deepseek_key
DEEPSEEK_BASE_URL=https://api.deepseek.com

# OpenAI (Optional - can be added later)
OPENAI_API_KEY=
OPENAI_ORG_ID=

# Anthropic (Optional - can be added later)
ANTHROPIC_API_KEY=

# Local LLM via Llamafile (Mistral 7B for sensitive data)
LLAMAFILE_ENABLED=true
LLAMAFILE_BASE_URL=http://localhost:8080
LLAMAFILE_MODEL=mistral-7b-instruct

# =============================================================================
# Database Configuration
# =============================================================================

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=breezer
POSTGRES_USER=breezer
POSTGRES_PASSWORD=change-me-in-production

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# Qdrant Vector DB
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=

# =============================================================================
# Agent Configuration
# =============================================================================

# Default model routing (using DeepSeek)
MODEL_IMPLEMENTATION=deepseek/deepseek-chat
MODEL_REVIEW=deepseek/deepseek-chat
MODEL_ARCHITECT=deepseek/deepseek-reasoner  # Reasoning model
MODEL_QA=deepseek/deepseek-chat
MODEL_DEBUG=deepseek/deepseek-reasoner
MODEL_DOCUMENTATION=deepseek/deepseek-chat
MODEL_REFACTORING=deepseek/deepseek-chat
MODEL_SECURITY=deepseek/deepseek-chat
MODEL_DEVOPS=deepseek/deepseek-chat

# Fallback model (local for sensitive data)
MODEL_FALLBACK=llamafile/mistral-7b-instruct

# Use local LLM for sensitive operations
USE_LOCAL_FOR_SENSITIVE=true

# =============================================================================
# Embeddings Configuration
# =============================================================================

# Use local GPU embeddings (recommended if you have RTX GPU)
EMBEDDINGS_PROVIDER=local  # or 'openai'
EMBEDDINGS_MODEL=sentence-transformers/all-mpnet-base-v2
EMBEDDINGS_DEVICE=cuda  # or 'cpu'

# If using OpenAI embeddings
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# =============================================================================
# Sandbox Configuration (Strategy B - Heavy Isolation)
# =============================================================================

SANDBOX_ENABLED=true
SANDBOX_STRATEGY=docker-in-docker  # Heavy isolation for security
SANDBOX_TIMEOUT=300  # seconds
SANDBOX_MEMORY_LIMIT=2g
SANDBOX_CPU_LIMIT=2
SANDBOX_NETWORK_ISOLATED=true  # No network access
SANDBOX_READ_ONLY_ROOT=true  # Read-only filesystem

# =============================================================================
# Service Configuration
# =============================================================================

# Backend API
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# CORS origins (comma-separated)
CORS_ORIGINS=vscode-webview://*,http://localhost:*

# =============================================================================
# Security
# =============================================================================

SECRET_KEY=generate-a-secure-random-key-here
JWT_ALGORITHM=HS256
JWT_EXPIRATION=86400  # 24 hours

# =============================================================================
# Telemetry (DISABLED - Company-wide sensitive data)
# =============================================================================

TELEMETRY_ENABLED=false
TELEMETRY_EXTERNAL_ENDPOINTS=  # Empty - no external telemetry
ANALYTICS_ENABLED=false
CRASH_REPORTING=false
USAGE_STATS=false

# All data stays on-premise
DATA_RESIDENCY=on-premise

# =============================================================================
# Logging
# =============================================================================

LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
LOG_FILE=logs/breezer.log

# =============================================================================
# Performance
# =============================================================================

# Context window limits
MAX_CONTEXT_TOKENS=128000
MAX_OUTPUT_TOKENS=4096

# Caching
CACHE_TTL=3600  # seconds
CACHE_ENABLED=true

# Rate limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_PERIOD=60  # seconds
